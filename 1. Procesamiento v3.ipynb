{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('ggplot')\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "# print(plt.style.available)\n",
    "from scipy.spatial import distance_matrix\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención de los conjuntos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accidentes con bicicletas implicadas (2019-2023)\n",
    "\n",
    "En primer lugar, se va a cargar el conjunto de datos de accidentes con bicicletas implicadas disponible en el portal de datos abiertos del Ayuntamiento de Madrid. Conviene destacar que los datos están disponibles tanto en formato .csv como .xlsx. La periodicidad de cada fichero es anual, si bien es cierto que hay un decalaje de algo más de un mes por lo que los datos más recientes son los del 31/05/2023.\n",
    "\n",
    "Respecto a la estructura de los datos, existe un registro por persona implicada en el accidente (es decir, que si en un accidente se han visto implicadas dos personas, habrá dos registros). Existe histórico desde 2010 hasta la actualidad, aunque en este caso se ha optado por capturar solo el histórico desde 2019 hasta la fecha, ya que en 2019 la estructura de los datos cambió. De esta forma, según la documentación disponible en la página web del portal de datos abiertos (https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=20f4a87ebb65b510VgnVCM1000001d4a900aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default), la estructura de los datos es la siguiente:\n",
    "* **num_expediente**: Número de experiente del accidente. Sigue el patrón AAAASNNNNNN, donde: AAAA es el año del accidente, S cuando se trata de un expediente con accidente y NNNNNN es un número correlativo por año. Como ya se comentó, puede haber más de un registro con el mismo número de expediente si ha habido varios afectados en un mismo accidente.\n",
    "* **fecha**: Fecha, en formato dd/mm/aaaa.\n",
    "* **hora**: La hora se establece en rangos horarios de 1 hora.\n",
    "* **localizacion**: Recoge el lugar del accidente. Suele seguir la estructura calle 1 ‐ calle 2 (para cruces) o una calle.\n",
    "* **numero**: Número de la calle, cuando la vía tiene sentido.\n",
    "* **cod_distrito**: Código único que sirve para identificar el distrito donde tuvo lugar el accidente.\n",
    "* **distrito**: Nombre del distrito.\n",
    "* **tipo_accidente**: Puede tomar los siguientes valores:\n",
    "    * **Colisión doble**: Accidente de tráfico ocurrido entre dos vehículos en movimiento, (colisión frontal, fronto lateral, lateral).\n",
    "    * **Colisión múltiple:**: Accidente de tráfico ocurrido entre más de dos vehículos en movimiento.\n",
    "    * **Alcance**: Accidente que se produce cuando un vehículo circulando o detenido por las circunstancias del tráfico es golpeado en su parte posterior por otro vehículo.\n",
    "    * **Choque contra obstáculo o elemento de la vía**: Accidente ocurrido entre un vehículo en movimiento con conductor y un objeto inmóvil que ocupa la vía o zona apartada de la misma, ya sea vehículo estacionado, árbol, farola, etc.\n",
    "    * **Atropello a persona**: Accidente ocurrido ente un vehículo y un peatón que ocupa la calzada o que transita por aceras, refugios, paseos o zonas de la vía pública no destinada a la circulación de vehículos.\n",
    "    * **Vuelco**: Accidente sufrido por un vehículo con más de dos ruedas y que por alguna circunstancia sus neumáticos pierden el contacto con la calzada quedando apoyado sobre un costado o sobre el techo.\n",
    "    * **Caída**: Se agrupan todas las caídas relacionadas con el desarrollo y las circunstancias del tráfico, (motocicleta, ciclomotor, bicicleta, viajero bus, etc.).\n",
    "    * **Otras causas**: Recoge los accidentes por atropello a animal, despeñamiento, salida de la vía, y otros.\n",
    "* **estado_meteorologico**: Condiciones ambientales presentes en el momento del accidente (nublado, despejado...).\n",
    "* **tipo_vehiculo**: Tipo de vehículo implicado (bicicleta, coche...).\n",
    "* **tipo_persona**: Rol de la persona implicada, a saber:: Conductor, peatón, testigo o viajero.\n",
    "* **rango_edad**: Tramo de edad de la persona implicada en el accidente.\n",
    "* **sexo**: Puede tomar los siguiente valores: Hombre, mujer o no asignado.\n",
    "* **cod_lesividad**: Código de la lesividad, tipoficado a continuación:\n",
    "    * 01: Atención en urgencias sin posterior ingreso. - LEVE\n",
    "    * 02: Ingreso inferior o igual a 24 horas - LEVE\n",
    "    * 03: Ingreso superior a 24 horas. - GRAVE\n",
    "    * 04: Fallecido 24 horas - FALLECIDO\n",
    "    * 05: Asistencia sanitaria ambulatoria con posterioridad - LEVE\n",
    "    * 06: Asistencia sanitaria inmediata en centro de salud o mutua - LEVE\n",
    "    * 07: Asistencia sanitaria sólo en el lugar del accidente - LEVE\n",
    "    * 14: Sin asistencia sanitaria\n",
    "    * 77: Se desconoce\n",
    "    * (En blanco): Sin asistencia sanitaria\n",
    "* **lesividad**: Descripción de la lesividad. Ver el campo anterior.\n",
    "* **coordenada_x_utm**: Coordenada X del lugar del accidente en el sistema de referencia de coordenadas ETRS89 / UTM zone 30N (EPSG: 25830).\n",
    "* **coordenada_y_utm**: Coordenada Y del lugar del accidente en el sistema de referencia de coordenadas ETRS89 / UTM zone 30N (EPSG: 25830).\n",
    "* **positiva_alcohol**: Indica si la persona involucrada dio positivo (S) en la prueba de alcohol o negativo (N).\n",
    "* **positiva_droga**: Indica si la persona involucrada dio positivo (1.0) en la prueba de drogas o negativo (en blanco).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_x_utm'] = df_temp['coordenada_x_utm'].str.replace('.', '').astype(float)/1000\n",
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:24: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_y_utm'] = df_temp['coordenada_y_utm'].str.replace('.', '').astype(float)/1000\n",
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_x_utm'] = df_temp['coordenada_x_utm'].str.replace('.', '').astype(float)/1000\n",
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:24: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_y_utm'] = df_temp['coordenada_y_utm'].str.replace('.', '').astype(float)/1000\n",
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_x_utm'] = df_temp['coordenada_x_utm'].str.replace('.', '').astype(float)/1000\n",
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:24: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_y_utm'] = df_temp['coordenada_y_utm'].str.replace('.', '').astype(float)/1000\n",
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_x_utm'] = df_temp['coordenada_x_utm'].str.replace('.', '').astype(float)/1000\n",
      "C:\\Users\\alke-\\AppData\\Local\\Temp\\ipykernel_8028\\673297701.py:24: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_temp['coordenada_y_utm'] = df_temp['coordenada_y_utm'].str.replace('.', '').astype(float)/1000\n"
     ]
    }
   ],
   "source": [
    "# Enlace para descargar cada dataset de accidentes con bicicletas implicadas\n",
    "accidentes_url = 'https://datos.madrid.es/egob/catalogo/300110-{id}-accidentes-bicicleta.csv'\n",
    "\n",
    "# Definimos una función para quitar las tíldes. Nos servirá para quitar las tíldes de los nombres de las columnas\n",
    "def quitar_tildes(texto):\n",
    "    tildes = {\n",
    "        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "        'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U'\n",
    "    }\n",
    "    texto_limpio = ''.join(tildes.get(c, c) for c in texto)\n",
    "    return texto_limpio\n",
    "\n",
    "# Empezamos cargando los datos de 2023 a partir del enlace web\n",
    "accidentes_raw_df = pd.read_csv(accidentes_url.format(id=26), delimiter=';', decimal=',')\n",
    "# Quitamos las tildes de los nombres de las columnas\n",
    "columnas_sin_tilde = [quitar_tildes(columna) for columna in accidentes_raw_df.columns]\n",
    "accidentes_raw_df.columns = columnas_sin_tilde\n",
    "\n",
    "# Ahora cargamos los demás datasets (de 2019 a 2022) y lo concatenamos al dataset inicial\n",
    "for id in range(18,26,2):\n",
    "    df_temp = pd.read_csv(accidentes_url.format(id=id), delimiter=';', decimal=',')\n",
    "    # Dado que hay un error en los datasets de 2019 a 2022 en las columnas de coordenadas, es necesario convertirlas de texto a número quitando los puntos y dividiendo entre 1000\n",
    "    df_temp['coordenada_x_utm'] = df_temp['coordenada_x_utm'].str.replace('.', '').astype(float)/1000\n",
    "    df_temp['coordenada_y_utm'] = df_temp['coordenada_y_utm'].str.replace('.', '').astype(float)/1000\n",
    "    # Se va a quitar las tíldes de los nombres de las columnas\n",
    "    columnas_sin_tilde = [quitar_tildes(columna) for columna in df_temp.columns]\n",
    "    df_temp.columns = columnas_sin_tilde\n",
    "    # Se unifican las columnas lesividad y tipo_lesividad en lesividad\n",
    "    if 'tipo_lesividad' in df_temp.columns:\n",
    "        df_temp['lesividad'] = df_temp['tipo_lesividad']\n",
    "        df_temp.drop(['tipo_lesividad'], axis=1, inplace=True)\n",
    "    # Concatenamos df_temp al dataset inicial\n",
    "    accidentes_raw_df = pd.concat([accidentes_raw_df, df_temp], ignore_index=True)\n",
    "\n",
    "# Se añaden una serie de campos que serán necesarios para futuros pasos\n",
    "# Se añade la columna que concatene el Año y el Mes\n",
    "accidentes_raw_df['AnoMes'] = pd.to_datetime(accidentes_raw_df['fecha'], format='%d/%m/%Y').dt.strftime('%Y-%m')\n",
    "\n",
    "# Se añade la columna que concatene la fecha y la hora, quitando los segundos y redondeando al cuarto de hora más próximo por defecto\n",
    "# El formato de esta columna será 2019-01-01 00:00:00\n",
    "accidentes_raw_df['fecha_hora'] = pd.to_datetime(accidentes_raw_df['fecha'] + ' ' + accidentes_raw_df['hora'], format='%d/%m/%Y %H:%M:%S').dt.floor('15min').dt.strftime('%Y-%m-%d %H:%M:00')\n",
    "\n",
    "# Se da el formato correcto a las columnas de coordenadas\n",
    "accidentes_raw_df['coordenada_x_utm'] = accidentes_raw_df['coordenada_x_utm'].astype(float)\n",
    "accidentes_raw_df['coordenada_y_utm'] = accidentes_raw_df['coordenada_y_utm'].astype(float)\n",
    "\n",
    "# Vamos a guardar el dataset en fichero CSV para tenerlo en local\n",
    "accidentes_raw_df.to_csv('./Datasets/accidentes_2019_2023.csv', sep=';', encoding='latin-1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4018 entries, 0 to 4017\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   num_expediente        4018 non-null   object \n",
      " 1   fecha                 4018 non-null   object \n",
      " 2   hora                  4018 non-null   object \n",
      " 3   localizacion          4018 non-null   object \n",
      " 4   numero                4018 non-null   object \n",
      " 5   cod_distrito          4018 non-null   int64  \n",
      " 6   distrito              4018 non-null   object \n",
      " 7   tipo_accidente        4018 non-null   object \n",
      " 8   estado_meteorologico  3892 non-null   object \n",
      " 9   tipo_vehiculo         4018 non-null   object \n",
      " 10  tipo_persona          4018 non-null   object \n",
      " 11  rango_edad            4018 non-null   object \n",
      " 12  sexo                  4018 non-null   object \n",
      " 13  cod_lesividad         3484 non-null   float64\n",
      " 14  lesividad             3484 non-null   object \n",
      " 15  coordenada_x_utm      4018 non-null   float64\n",
      " 16  coordenada_y_utm      4018 non-null   float64\n",
      " 17  positiva_alcohol      4011 non-null   object \n",
      " 18  positiva_droga        8 non-null      float64\n",
      " 19  AnoMes                4018 non-null   object \n",
      " 20  fecha_hora            4018 non-null   object \n",
      "dtypes: float64(4), int64(1), object(16)\n",
      "memory usage: 659.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Se realizan varias comprobaciones sobre el dataset, como ver su información básica\n",
    "accidentes_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_expediente             0\n",
       "fecha                      0\n",
       "hora                       0\n",
       "localizacion               0\n",
       "numero                     0\n",
       "cod_distrito               0\n",
       "distrito                   0\n",
       "tipo_accidente             0\n",
       "estado_meteorologico     126\n",
       "tipo_vehiculo              0\n",
       "tipo_persona               0\n",
       "rango_edad                 0\n",
       "sexo                       0\n",
       "cod_lesividad            534\n",
       "lesividad                534\n",
       "coordenada_x_utm           0\n",
       "coordenada_y_utm           0\n",
       "positiva_alcohol           7\n",
       "positiva_droga          4010\n",
       "AnoMes                     0\n",
       "fecha_hora                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos de forma más sencilla donde hay valores nulos\n",
    "accidentes_raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cod_distrito</th>\n",
       "      <th>cod_lesividad</th>\n",
       "      <th>coordenada_x_utm</th>\n",
       "      <th>coordenada_y_utm</th>\n",
       "      <th>positiva_droga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4018.000000</td>\n",
       "      <td>3484.000000</td>\n",
       "      <td>4018.000000</td>\n",
       "      <td>4.018000e+03</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.331010</td>\n",
       "      <td>5.837830</td>\n",
       "      <td>441649.178399</td>\n",
       "      <td>4.475131e+06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.800212</td>\n",
       "      <td>3.898194</td>\n",
       "      <td>3354.793170</td>\n",
       "      <td>3.750375e+03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>430367.536000</td>\n",
       "      <td>4.464458e+06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>439681.716750</td>\n",
       "      <td>4.472901e+06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>441140.659500</td>\n",
       "      <td>4.474747e+06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>443217.807250</td>\n",
       "      <td>4.476912e+06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>454614.270000</td>\n",
       "      <td>4.490695e+06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cod_distrito  cod_lesividad  coordenada_x_utm  coordenada_y_utm  \\\n",
       "count   4018.000000    3484.000000       4018.000000      4.018000e+03   \n",
       "mean       8.331010       5.837830     441649.178399      4.475131e+06   \n",
       "std        5.800212       3.898194       3354.793170      3.750375e+03   \n",
       "min        1.000000       1.000000     430367.536000      4.464458e+06   \n",
       "25%        3.000000       2.000000     439681.716750      4.472901e+06   \n",
       "50%        8.000000       6.000000     441140.659500      4.474747e+06   \n",
       "75%       13.000000       7.000000     443217.807250      4.476912e+06   \n",
       "max       21.000000      14.000000     454614.270000      4.490695e+06   \n",
       "\n",
       "       positiva_droga  \n",
       "count             8.0  \n",
       "mean              1.0  \n",
       "std               0.0  \n",
       "min               1.0  \n",
       "25%               1.0  \n",
       "50%               1.0  \n",
       "75%               1.0  \n",
       "max               1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos rápidamente que los campos numéricos tienen los valores esperados\n",
    "accidentes_raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se observa que las columnas de coordenadas toman valores coherentes para ser coordenadas utm\n",
    "# Esto es clave ya que luego se van a utilizar estos campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conductor    3734\n",
       "Peatón        263\n",
       "Pasajero       21\n",
       "Name: tipo_persona, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidentes_raw_df.tipo_persona.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se añade el número de implicados\n",
    "Esto viene de 1. Procesamiento v2 desde el inicio hasta la celda que pone HASTA AQUÍ LO QUE SE PUEDE APROVECHAR PARA 1. Procesamiento.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicados = pd.read_csv('./Datasets/accidentes_con_implicados.csv', delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Caída                           1327\n",
       "Colisión fronto-lateral          959\n",
       "Alcance                          549\n",
       "Atropello a persona              482\n",
       "Colisión lateral                 420\n",
       "Colisión frontal                 189\n",
       "Choque contra obstáculo fijo     119\n",
       "Otro                              34\n",
       "Atropello a animal                25\n",
       "Colisión múltiple                 18\n",
       "Vuelco                             5\n",
       "Solo salida de la vía              3\n",
       "Name: tipo_accidente, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicados.tipo_accidente.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicados = implicados.drop_duplicates(subset='num_expediente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'num_expediente', 'fecha', 'hora', 'localizacion', 'numero',\n",
       "       'cod_distrito', 'distrito', 'tipo_accidente', 'estado_meteorologico',\n",
       "       'tipo_vehiculo', 'tipo_persona', 'rango_edad', 'sexo', 'cod_lesividad',\n",
       "       'lesividad', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_alcohol',\n",
       "       'positiva_droga', 'AnoMes', 'fecha_hora', 'tipo_vehiculo_agg',\n",
       "       'implicados', 'implicados_peatones', 'implicados_pesados',\n",
       "       'implicados_turismos', 'implicados_motocicletas',\n",
       "       'implicados_bicicletas', 'implicados_otros', 'implicados_epac',\n",
       "       'implicados_ligeros'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicados.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicados = implicados[['num_expediente', 'implicados', 'implicados_peatones', 'implicados_pesados', 'implicados_turismos',\n",
    "       'implicados_motocicletas', 'implicados_bicicletas', 'implicados_otros',\n",
    "       'implicados_epac', 'implicados_ligeros']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidentes_raw_df = pd.merge(accidentes_raw_df,implicados, on=['num_expediente'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4018 entries, 0 to 4017\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   num_expediente           4018 non-null   object \n",
      " 1   fecha                    4018 non-null   object \n",
      " 2   hora                     4018 non-null   object \n",
      " 3   localizacion             4018 non-null   object \n",
      " 4   numero                   4018 non-null   object \n",
      " 5   cod_distrito             4018 non-null   int64  \n",
      " 6   distrito                 4018 non-null   object \n",
      " 7   tipo_accidente           4018 non-null   object \n",
      " 8   estado_meteorologico     3892 non-null   object \n",
      " 9   tipo_vehiculo            4018 non-null   object \n",
      " 10  tipo_persona             4018 non-null   object \n",
      " 11  rango_edad               4018 non-null   object \n",
      " 12  sexo                     4018 non-null   object \n",
      " 13  cod_lesividad            3484 non-null   float64\n",
      " 14  lesividad                3484 non-null   object \n",
      " 15  coordenada_x_utm         4018 non-null   float64\n",
      " 16  coordenada_y_utm         4018 non-null   float64\n",
      " 17  positiva_alcohol         4011 non-null   object \n",
      " 18  positiva_droga           8 non-null      float64\n",
      " 19  AnoMes                   4018 non-null   object \n",
      " 20  fecha_hora               4018 non-null   object \n",
      " 21  implicados               4018 non-null   int64  \n",
      " 22  implicados_peatones      4018 non-null   float64\n",
      " 23  implicados_pesados       4018 non-null   float64\n",
      " 24  implicados_turismos      4018 non-null   float64\n",
      " 25  implicados_motocicletas  4018 non-null   float64\n",
      " 26  implicados_bicicletas    4018 non-null   float64\n",
      " 27  implicados_otros         4018 non-null   float64\n",
      " 28  implicados_epac          4018 non-null   float64\n",
      " 29  implicados_ligeros       4018 non-null   float64\n",
      "dtypes: float64(12), int64(2), object(16)\n",
      "memory usage: 973.1+ KB\n"
     ]
    }
   ],
   "source": [
    "accidentes_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: tipo_persona, dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidentes_raw_df[accidentes_raw_df['implicados'].isna()].tipo_persona.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mejora del campo localización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación, se va a proceder a mejorar el campo localizacion, de cara a obtener información útil al respecto\n",
    "# Lo primero que se va a hacer es eliminar las tildes del campo y convertir todo a mayúsculas\n",
    "accidentes_raw_df[\"localizacion\"] = accidentes_raw_df[\"localizacion\"].apply(quitar_tildes).str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seguidamente, se van a quitar los espacios antes y después del texto\n",
    "accidentes_raw_df[\"localizacion\"] = accidentes_raw_df[\"localizacion\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tras revisar los registros, se observa que se pueden eliminar los puntos ya que no aportan nada\n",
    "accidentes_raw_df[\"localizacion\"] = accidentes_raw_df[\"localizacion\"].apply(lambda x: re.sub(r'\\.', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También se van a quitar los espacios en blanco de más\n",
    "accidentes_raw_df[\"localizacion\"] = accidentes_raw_df[\"localizacion\"].apply(lambda x: re.sub(r'\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un 43.55 % de los registros tienen como localización un cruce\n"
     ]
    }
   ],
   "source": [
    "# Se va a crear un filtro para ver qué registros contienen \" - \", \" / \", \" -- \", \" CON \"\n",
    "filtro = accidentes_raw_df[\"localizacion\"].str.contains(r' - | -- | / |\\sCON\\s', case=False, regex=True)\n",
    "\n",
    "# Se muestra qué porcentaje de registros cumplen el patrón y, por tanto, son accidentes en cruces\n",
    "print(f\"Un {(accidentes_raw_df[filtro].count()[0]/ accidentes_raw_df.count()[0])*100:.2f} % de los registros tienen como localización un cruce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para simplificar el tratamiento, se va a separar la columna localización es dos columnas a partir de los patrones \" / \", \" - \", \" -- \" o \" CON \"\n",
    "accidentes_raw_df[['localizacion_1', 'localizacion_2']] = accidentes_raw_df[\"localizacion\"].str.split(r' - | -- | / |\\sCON\\s', expand=True, n=1)\n",
    "# Se rellenan los vacíos de la segunda columna con (vacío) para facilitar las funciones con texto\n",
    "accidentes_raw_df['localizacion_2'].fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se vuelven a quitar los espacios de más y los espacios antes y después del texto, en este caso para las dos columnas nuevas\n",
    "accidentes_raw_df[\"localizacion_1\"] = accidentes_raw_df[\"localizacion_1\"].str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "accidentes_raw_df[\"localizacion_2\"] = accidentes_raw_df[\"localizacion_2\"].str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación, se quiere obtener para cada localización el tipo de vía, el nombre de la vía y el número\n",
    "# Para ello, se va a definir una función que permita limpiar la dirección y otra que analice la dirección resultante para obtener el tipo de vía, su nombre y el número\n",
    "                       \n",
    "def limpiar_direccion(direccion):\n",
    "    \"\"\"\n",
    "    Función que limpia la dirección, quitando una serie de stopwords, los textos entre paréntesis, el símbolo + y algún otro patrón como que empiece por la palabra FRENTE\n",
    "    \"\"\"\n",
    "    # Se eliminan las stopwords y palabras específicas\n",
    "    stopwords = [\"S/N\", \"NUM\", \"KM\", \"PK\", \"Nº\"]\n",
    "    direccion = re.sub(r'\\b(?:' + '|'.join(map(re.escape, stopwords)) + r')\\b', '', direccion, flags=re.IGNORECASE)\n",
    "    direccion = re.sub(r'\\bCRUCE\\b', '', direccion, flags=re.IGNORECASE)\n",
    "\n",
    "    # Se elimina el texto entre paréntesis\n",
    "    direccion = re.sub(r'\\(.*?\\)', '', direccion)\n",
    "\n",
    "    # Se elimina el símbolo +\n",
    "    direccion = re.sub(r'\\+', '', direccion)\n",
    "\n",
    "    # Se separa la dirección por palabras\n",
    "    palabras = direccion.split()\n",
    "\n",
    "    # Se busca la posición de \"FRENTE\"\n",
    "    indice_frente = palabras.index(\"FRENTE\") if \"FRENTE\" in palabras else -1\n",
    "\n",
    "    # Se elimina todo lo que aparece después de \"FRENTE\" solo si \"FRENTE\" no es la primera palabra, ya que si no, eliminamos la dirección completa\n",
    "    if indice_frente > 0:\n",
    "        direccion = ' '.join(palabras[:indice_frente])\n",
    "\n",
    "    # Se elimina todo lo que aparece después de \"FRENTE\"\n",
    "    #direccion = re.split(r'\\bFRENTE\\b', direccion, flags=re.IGNORECASE)[0]\n",
    "\n",
    "    return direccion.strip()\n",
    "\n",
    "def analizar_direccion(direccion):\n",
    "    \"\"\"\n",
    "    Función que a partir de una dirección, la analiza para obtener el tipo de vía, el nombre y el número de la misma\n",
    "    \"\"\"\n",
    "    # Se convierte todo a mayúscula\n",
    "    direccion = direccion.upper()\n",
    "\n",
    "    # Condición para evitar errores de direcciones vacías\n",
    "    if len(direccion) < 2:\n",
    "        return None, None, None\n",
    "\n",
    "    # Se definen las variables\n",
    "    tipo_via = None\n",
    "    patron_via = None\n",
    "\n",
    "    # Se definen los patrones posibles\n",
    "    patrones = {\n",
    "        \"CALLE\": [\"CALLE\", \"CALL\", \"CL\", \"C/\"],\n",
    "        \"AVENIDA\": [\"AVENIDA\", \"AVDA\", \"AVD\", \"AV\"],\n",
    "        \"PLAZA\": [\"PLAZA\"],\n",
    "        \"ARROYO\": [\"ARROYO\", \"ARRY\"],\n",
    "        \"AUTOPISTA\": [\"AUTOPISTA\", \"AUTOP\"],\n",
    "        \"AUTOVÍA\": [\"AUTOVÍA\", \"AUTOV\"],\n",
    "        \"BULEVAR\": [\"BULEV\"],\n",
    "        \"CARRETERA\": [\"CARRETERA\", \"CARETAR\", \"CRTA\", \"CTRA\", \"CRA\"],\n",
    "        \"ACCESO\": [\"ACCESO\"],\n",
    "        \"PASEO\": [\"PASEO\"],\n",
    "        \"GLORIETA\": [\"GLORIETA\", \"GTA\"],\n",
    "        \"PLAZA\": [\"PLAZA\", \"PLZA\", \"PZA\"],\n",
    "        \"RONDA\": [\"RONDA\", \"ROND\", \"RDA\"],\n",
    "        \"COSTANILLA\": [\"COSTANILLA\", \"CSTAN\"],\n",
    "        \"CAMINO\": [\"CAMINO\", \"CMNO\"],\n",
    "        \"CAÑADA\": [\"CAÑADA\", \"CÑADA\", \"CNADA\"],\n",
    "        \"CUESTA\": [\"CUESTA\", \"CUSTA\"],\n",
    "        \"EDIFICIO\": [\"EDIFICIO\", \"EDIFI\"],\n",
    "        \"PUENTE\": [\"PUENTE\", \"PNTE\", \"PTE\"],\n",
    "        \"LUGAR\": [\"LUGAR\"],\n",
    "        \"PISTA\": [\"PISTA\"],\n",
    "        \"POLÍGONO\": [\"POLIGONO\", \"POLIG\"],\n",
    "        \"PARQUE\": [\"PARQUE\", \"PQUE\"],\n",
    "        \"PUERTA\": [\"PUERTA\", \"PTA\"],\n",
    "        \"TRAVESÍA\": [\"TRAVESIA\", \"TRVA\"]\n",
    "    }\n",
    "    \n",
    "    # Se limpia la dirección\n",
    "    direccion_limpia = limpiar_direccion(direccion)\n",
    "\n",
    "    # Se buscan los patrones en la direccion\n",
    "    for t_via, lista_patrones in patrones.items():\n",
    "        for patron in lista_patrones:\n",
    "            if re.match(f\"^{patron}$\", direccion_limpia.split()[0], re.IGNORECASE):\n",
    "                tipo_via = t_via\n",
    "                patron_via = patron\n",
    "                break\n",
    "    \n",
    "    # Se definen más variables\n",
    "    nombre_via = direccion_limpia\n",
    "    numero_via = None\n",
    "\n",
    "    # Se elimina el patrón de tipo de vía al inicio\n",
    "    if patron_via:\n",
    "        nombre_via = re.sub(rf\"^{patron_via}\\s*\", '', nombre_via, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Se elimina \"DE\" y \"DEL\" del inicio del texto\n",
    "    nombre_via = re.sub(r'^\\bDE\\b|\\bDEL\\b', '', nombre_via, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    if tipo_via in [\"CARRETERA\", \"AUTOPISTA\", \"AUTOVÍA\"]:\n",
    "        # Se busca el patrón \"M-\", \"A-\", \"N-\" o \"R-\" seguido de uno o varios números\n",
    "        nombre_match = re.search(r'^(M-|A-|N-|R-)\\d+', direccion_limpia.split(',')[0])\n",
    "        if nombre_match:\n",
    "            nombre_via = nombre_match.group()\n",
    "            direccion_sin_nombre = re.sub(rf\"{nombre_via}\", '', direccion_limpia).strip()\n",
    "            numero_match = re.search(r'(?:\\+)?\\d+', direccion_sin_nombre)\n",
    "            if numero_match:\n",
    "                numero_via = numero_match.group()\n",
    "    else:\n",
    "        # Se busca el número de la vía\n",
    "        numero_match = re.search(r'(?:\\+)?\\d+', direccion_limpia)\n",
    "        if numero_match:\n",
    "            numero_via = numero_match.group()\n",
    "            # nombre_via = direccion_limpia.replace(numero_via, '').strip()\n",
    "            nombre_via = re.sub(rf\"{numero_via}.*\", '', nombre_via, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Se eliminan las comas al final del texto\n",
    "    nombre_via = re.sub(r',\\s*', '', nombre_via.strip())\n",
    "\n",
    "    # Se eliminan los espacios en blanco múltiples\n",
    "    nombre_via = re.sub(r'\\s+', ' ', nombre_via)\n",
    "\n",
    "    # Se eliminan los espacios al inicio y al final\n",
    "    nombre_via = nombre_via.strip()\n",
    "\n",
    "    # Se vuelve a limpiar el nombre_via\n",
    "    nombre_via = limpiar_direccion(nombre_via)\n",
    "    \n",
    "    return tipo_via, nombre_via, numero_via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seguidamente, se aplica a las columnas localizacion_1 y localizacion_2\n",
    "accidentes_raw_df[['clase_via_1','nombre_via_1', 'numero_via_1']] = accidentes_raw_df[\"localizacion_1\"] .apply(lambda x: pd.Series(analizar_direccion(x)))\n",
    "accidentes_raw_df[['clase_via_2','nombre_via_2', 'numero_via_2']] = accidentes_raw_df[\"localizacion_2\"] .apply(lambda x: pd.Series(analizar_direccion(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a guardar el dataset en fichero CSV para tenerlo en local\n",
    "accidentes_raw_df.to_csv('./Datasets/accidentes_con_vias_2019_2023.csv', sep=';', encoding='latin-1', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ubicación de los puntos de medida del tráfico (2019-2023)\n",
    "\n",
    "Una vez cargados los accidentes con implicación de bicicletas desde el 2019 hasta la actualidad, se echa de menos algún atributo que indique la intensidad del tráfico para cada accidente. Por ello, se opta por enriquecer el conjunto de datos anterior con el histórico de tráfico que tiene disponible el Portal abierto del Ayuntamiento de Madrid. Como paso previo a añadir una columna que especifique la intensidad de tráfico en el lugar y momento del accidente, es necesario identificar el punto de medida del tráfico más próximo a cada lugar del accidente en el instante en el que tuvo lugar.\n",
    "\n",
    "De esta forma, se carga el conjunto de datos de los puntos de medida del tráfico accesible desde la página web del Portal abierto del Ayuntamiento de Madrid. De forma mensual, en esta página web se cuelga la localización y la información básica de los distintos puntos de medida existentes en la ciudad de Madrid. La información está disponible tanto en formato shapefile (SHP) como csv, y su contenido es el siguiente:\n",
    "* **tipo_elem**: Permite distinguir entre dispositivos de control semafórico (URB) y dispositivos de vías rápidas y accesos a Madrid (M-30).\n",
    "* **distrito**: Código único que sirve para identificar el distrito donde se ubica el punto de medida.\n",
    "* **id**: Identificador secuencial, único e invariable del punto de medida. Este es el campo que permitirá relacionar cada accidente con la intensidad de tráfico.\n",
    "* **cod_cent**: Código centralizado del punto de medida.\n",
    "* **nombre**: La denominación de los puntos de medida sigue una nomenclatura común:\n",
    "    * Para los puntos de medida del tráfico en áreas urbanas, se utiliza la calle y la dirección del flujo de circulación.\n",
    "    * Para los puntos de medición del tráfico en vías rápidas y accesos a Madrid, se emplea el punto kilométrico, la calzada y se indica si es la vía central, la vía de servicio o un enlace.\n",
    "* **utm_x**: Coordenada X del punto que indica la representación del punto de medida en el sistema de referencia de coordenadas ETRS89 / UTM zone 30N (EPSG: 25830).\n",
    "* **utm_y**: Coordenada Y del punto que indica la representación del punto de medida en el sistema de referencia de coordenadas ETRS89 / UTM zone 30N (EPSG: 25830).\n",
    "* **longitud**: Longitud en el sistema de referencia de coordenadas WGS 84 (EPSG:4326).\n",
    "* **latitud**: Latitud en el sistema de referencia de coordenadas WGS 84 (EPSG:4326)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlace para descargar cada dataset con la ubicación de los puntos de medida del tráfico\n",
    "pmed_url = 'https://datos.madrid.es/egob/catalogo/202468-{id}-intensidad-trafico.csv'\n",
    "\n",
    "# Empezamos cargando los datos de enero de 2019 (2019-01), ya que los conjuntos de datos están disponibles con periodicidad mensual\n",
    "pmed_raw_df = pd.read_csv(pmed_url.format(id=31), delimiter=';', encoding='utf-8', decimal='.')\n",
    "\n",
    "# Añadimos la columna que nos ayudará a identificar el año-mes de los puntos de medida del tráfico\n",
    "pmed_raw_df['AnoMes'] = '2019-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A diferencia del conjunto de datos de accidentes con bicicletas implicadas, el id de la url no sigue un orden coherente, por lo que\n",
    "# se va a definir un diccionario que relacione cada id con el año-mes\n",
    "diccionario_pmed = {\n",
    "    31:'2019-01',\n",
    "    34:'2019-02',\n",
    "    37:'2019-03',\n",
    "    40:'2019-04',\n",
    "    43:'2019-05',\n",
    "    46:'2019-06',\n",
    "    49:'2019-07',\n",
    "    52:'2019-08',\n",
    "    55:'2019-09',\n",
    "    58:'2019-10',\n",
    "    61:'2019-11',\n",
    "    64:'2019-12',\n",
    "    67:'2020-01',\n",
    "    71:'2020-02',\n",
    "    68:'2020-03',\n",
    "    74:'2020-04',\n",
    "    77:'2020-05',\n",
    "    80:'2020-06',\n",
    "    83:'2020-07',\n",
    "    86:'2020-08',\n",
    "    89:'2020-09',\n",
    "    92:'2020-10',\n",
    "    95:'2020-11',\n",
    "    98:'2020-12',\n",
    "    101:'2021-01',\n",
    "    104:'2021-02',\n",
    "    107:'2021-03',\n",
    "    110:'2021-04',\n",
    "    113:'2021-05',\n",
    "    116:'2021-06',\n",
    "    119:'2021-07',\n",
    "    122:'2021-08',\n",
    "    126:'2021-09',\n",
    "    128:'2021-10',\n",
    "    131:'2021-11',\n",
    "    134:'2021-12',\n",
    "    137:'2022-01',\n",
    "    140:'2022-02',\n",
    "    143:'2022-03',\n",
    "    146:'2022-04',\n",
    "    149:'2022-05',\n",
    "    152:'2022-06',\n",
    "    155:'2022-07',\n",
    "    158:'2022-08',\n",
    "    161:'2022-09',\n",
    "    164:'2022-10',\n",
    "    167:'2022-11',\n",
    "    170:'2022-12',\n",
    "    173:'2023-01',\n",
    "    176:'2023-02',\n",
    "    179:'2023-03',\n",
    "    182:'2023-04',\n",
    "    185:'2023-05',\n",
    "    188:'2023-06'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01 utf-8\n",
      "2019-02 utf-8\n",
      "2019-03 utf-8\n",
      "2019-04 utf-8\n",
      "2019-05 latin-1\n",
      "2019-06 latin-1\n",
      "2019-07 utf-8\n",
      "2019-08 utf-8\n",
      "2019-09 utf-8\n",
      "2019-10 utf-8\n",
      "2019-11 utf-8\n",
      "2019-12 utf-8\n",
      "2020-01 utf-8\n",
      "2020-02 utf-8\n",
      "2020-03 utf-8\n",
      "2020-04 utf-8\n",
      "2020-05 utf-8\n",
      "2020-06 utf-8\n",
      "2020-07 utf-8\n",
      "2020-08 utf-8\n",
      "2020-09 utf-8\n",
      "2020-10 utf-8\n",
      "2020-11 utf-8\n",
      "2020-12 utf-8\n",
      "2021-01 utf-8\n",
      "2021-02 utf-8\n",
      "2021-03 utf-8\n",
      "2021-04 utf-8\n",
      "2021-05 utf-8\n",
      "2021-06 utf-8\n",
      "2021-07 utf-8\n",
      "2021-08 utf-8\n",
      "2021-09 utf-8\n",
      "2021-10 utf-8\n",
      "2021-11 utf-8\n",
      "2021-12 utf-8\n",
      "2022-01 utf-8\n",
      "2022-02 utf-8\n",
      "2022-03 utf-8\n",
      "2022-04 utf-8\n",
      "2022-05 utf-8\n",
      "2022-06 utf-8\n",
      "2022-07 utf-8\n",
      "2022-08 utf-8\n",
      "2022-09 utf-8\n",
      "2022-10 utf-8\n",
      "2022-11 utf-8\n",
      "2022-12 utf-8\n",
      "2023-01 utf-8\n",
      "2023-02 utf-8\n",
      "2023-03 utf-8\n",
      "2023-04 utf-8\n",
      "2023-05 utf-8\n",
      "2023-06 utf-8\n"
     ]
    }
   ],
   "source": [
    "# Probramos con diferentes codificaciones hasta encontrar la correcta para cada dataset, ya que se ha observado que\n",
    "# la codificación cambia para cada mes\n",
    "encodings = ['utf-8', 'latin-1', 'ISO-8859-1']\n",
    "\n",
    "# Ahora cargamos los demás datasets (de 2019-02 a 2023-06) y lo concatenamos al dataset inicial\n",
    "for x in diccionario_pmed:\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df_temp = pd.read_csv(pmed_url.format(id=x), delimiter=';', encoding=encoding, decimal='.')\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    df_temp['AnoMes'] = diccionario_pmed[x]\n",
    "    pmed_raw_df = pd.concat([pmed_raw_df, df_temp], ignore_index=True)\n",
    "    # Mostramos en pantalla el año-mes que se está cargando, y el encoding aplicado\n",
    "    print(diccionario_pmed[x], encoding)\n",
    "\n",
    "# Vamos a guardar el dataset en fichero CSV para tenerlo en local\n",
    "pmed_raw_df.to_csv('./Datasets/pmed_ubicacion_2019_2023.csv', sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distrito</th>\n",
       "      <th>id</th>\n",
       "      <th>utm_x</th>\n",
       "      <th>utm_y</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>243912.000000</td>\n",
       "      <td>244149.000000</td>\n",
       "      <td>244149.000000</td>\n",
       "      <td>2.441490e+05</td>\n",
       "      <td>244149.000000</td>\n",
       "      <td>244149.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.822018</td>\n",
       "      <td>6209.274148</td>\n",
       "      <td>441714.313788</td>\n",
       "      <td>4.475644e+06</td>\n",
       "      <td>-3.687142</td>\n",
       "      <td>40.429381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.302096</td>\n",
       "      <td>2295.241488</td>\n",
       "      <td>3474.518011</td>\n",
       "      <td>4.190451e+03</td>\n",
       "      <td>0.040898</td>\n",
       "      <td>0.037789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>429055.947346</td>\n",
       "      <td>4.464902e+06</td>\n",
       "      <td>-3.836943</td>\n",
       "      <td>40.332454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>4534.000000</td>\n",
       "      <td>439491.253385</td>\n",
       "      <td>4.472310e+06</td>\n",
       "      <td>-3.713128</td>\n",
       "      <td>40.399064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>5692.000000</td>\n",
       "      <td>441469.594638</td>\n",
       "      <td>4.475791e+06</td>\n",
       "      <td>-3.690123</td>\n",
       "      <td>40.430855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>6839.000000</td>\n",
       "      <td>443993.831389</td>\n",
       "      <td>4.478871e+06</td>\n",
       "      <td>-3.660338</td>\n",
       "      <td>40.458456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>11127.000000</td>\n",
       "      <td>450772.278972</td>\n",
       "      <td>4.485213e+06</td>\n",
       "      <td>-3.580713</td>\n",
       "      <td>40.515611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            distrito             id          utm_x         utm_y  \\\n",
       "count  243912.000000  244149.000000  244149.000000  2.441490e+05   \n",
       "mean        9.822018    6209.274148  441714.313788  4.475644e+06   \n",
       "std         5.302096    2295.241488    3474.518011  4.190451e+03   \n",
       "min         1.000000       0.000000  429055.947346  4.464902e+06   \n",
       "25%         5.000000    4534.000000  439491.253385  4.472310e+06   \n",
       "50%         9.000000    5692.000000  441469.594638  4.475791e+06   \n",
       "75%        15.000000    6839.000000  443993.831389  4.478871e+06   \n",
       "max        21.000000   11127.000000  450772.278972  4.485213e+06   \n",
       "\n",
       "            longitud        latitud  \n",
       "count  244149.000000  244149.000000  \n",
       "mean       -3.687142      40.429381  \n",
       "std         0.040898       0.037789  \n",
       "min        -3.836943      40.332454  \n",
       "25%        -3.713128      40.399064  \n",
       "50%        -3.690123      40.430855  \n",
       "75%        -3.660338      40.458456  \n",
       "max        -3.580713      40.515611  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Al igual que se hizo en el dataset anterior, comprobamos rápidamente que los datos numéricos tienen los valores esperados\n",
    "pmed_raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 293608 entries, 0 to 293607\n",
      "Data columns (total 10 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   tipo_elem  244149 non-null  object \n",
      " 1   distrito   243912 non-null  float64\n",
      " 2   id         244149 non-null  float64\n",
      " 3   cod_cent   244149 non-null  object \n",
      " 4   nombre     243461 non-null  object \n",
      " 5   utm_x      244149 non-null  float64\n",
      " 6   utm_y      244149 non-null  float64\n",
      " 7   longitud   244149 non-null  float64\n",
      " 8   latitud    244149 non-null  float64\n",
      " 9   AnoMes     293608 non-null  object \n",
      "dtypes: float64(6), object(4)\n",
      "memory usage: 22.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Se analiza el tipo de cada columna, así como los nulos de cada columna\n",
    "pmed_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se confirma que las columnas de coordenadas tiene el formato correcto, por lo que ya se puede avanzar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histórico de datos del tráfico desde 2019\n",
    "\n",
    "Hasta ahora, si despone de un conjunto de datos con todos los accidentes con implicación de bicicletas, y otro, con la ubicación de los puntos de medida por mes, para el periodo completo de análisis (de 2019 hasta la actualidad). Llegados a este punto, solo queda añadir a cada accidente el histórico de datos de tráfico teniendo en cuenta la fecha y hora del accidente, así como el id del punto de medida más próximo. Al igual que hasta hora, desde el portal de datos abiertos del Ayuntamiento de Madrid se puede descargar esta información. \n",
    "\n",
    "De esta forma, el conjunto de datos que hay que cargar es el de datos históricos de los puntos de medida de tráfico. En este caso, el decalaje es de un mes, y los datos se capturan en los distintos puntos de medida y se integran en la base de datos SICTRAM cada 15 minutos. Los datos están disponibles en ficheros csv mensuales, cuya estructura es la siguiente:\n",
    "\n",
    "* **id**: Identificador secuencial, único e invariable del punto de medida. Este es el campo que permitirá relacionar cada registro con el punto de medida de tráfico correspondiente.\n",
    "* **fecha**: Fecha y hora oficiales de Madrid con formato dd/mm/yyyy hh:mi:ss\n",
    "* **tipo_elem**: Permite distinguir entre dispositivos de control semafórico o urbanos (URB) y dispositivos de vías rápidas y accesos a Madrid (M-30).\n",
    "* **intensidad**: Número de vehículos que discurren por un punto de medida en cada hora. Dado que sus unidades son vehículos/hora, para obtener el valor efectivo de vehículos que han circulado en ese intervalo de 15 minutos, hay que dividir entre cuatro el número disponible. Un valor negativo se interpreta como la ausencia de datos.\n",
    "* **ocupacion**: Porcentaje de tiempo que está un punto de medida de tráfico ocupado por un vehículo. De esta forma, si un vehículo ha estado situado durante 7,5 minutos frente a un punto de medida, la ocupación registrada en un periodo de 15 minutos será del 50%. Un valor negativo se interpreta como la ausencia de datos.\n",
    "* **carga**: Parámetro de carga de la vía en el intervalo de 15 minutos. Se trata de una medida sintética utilizada para estimar el nivel de congestión, calculado a partir de un algoritmo que usa como variables la intensidad y la ocupación, con ciertos factores de corrección. Establece el grado de uso de la vía en un rango de 0 (vacía) a 100 (colapso). Un valor negativo significa la ausencia de datos. En puntos de medida de tipo M-30 no se utiliza el parámetro de carga, ya que no hay regulación semafórica, por lo que su valor será nulo.\n",
    "* **vmed**: Velocidad media de los vehículos en el periodo de 15 minutos (km/h). Solo para puntos de medida interurbanos M30. Un valor negativo implica la ausencia de datos.\n",
    "* **error**: Variable que indica si ha habido al menos una muestra errónea o sustituida en el laps de 15 minutos. Puede tomar los siguientes valores:\n",
    "    * N: sin errores ni sustituciones.\n",
    "    * E: la calidad de alguna de las muestras integradas no es óptima.\n",
    "    * S: alguna de las muestras recibidas era totalmente errónea y no se ha integrado.\n",
    "* **periodo_integracion**: Cantidad de muestras recibidas y consideradas para el periodo de integración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlace para descargar cada dataset con el histórico de datos del tráfico desde 2019\n",
    "trafico_url = 'https://datos.madrid.es/egob/catalogo/208627-{id}-transporte-ptomedida-historico.zip'\n",
    "\n",
    "# Empezamos cargando los datos de 2019-01\n",
    "response = requests.get(trafico_url.format(id=68))\n",
    "\n",
    "# En este caso, la descarga es un fichero zip\n",
    "with zipfile.ZipFile(BytesIO(response.content), 'r') as zip_ref:\n",
    "    # Dado que solo hay un archivo csv en el zip, seleccionamos el primero\n",
    "    csv_file_name = zip_ref.namelist()[0]\n",
    "\n",
    "    # Leemos el archivo csv que hay dentro del zip\n",
    "    with zip_ref.open(csv_file_name) as csv_file:\n",
    "        # Cargamos el archivo csv directamente con pandas\n",
    "        trafico_raw_df = pd.read_csv(csv_file, delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11203627 entries, 0 to 11203626\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Dtype  \n",
      "---  ------               -----  \n",
      " 0   id                   int64  \n",
      " 1   fecha                object \n",
      " 2   tipo_elem            object \n",
      " 3   intensidad           int64  \n",
      " 4   ocupacion            float64\n",
      " 5   carga                int64  \n",
      " 6   vmed                 float64\n",
      " 7   error                object \n",
      " 8   periodo_integracion  int64  \n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 769.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Se comprueba la información básica del conjunto de datos cargado\n",
    "trafico_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fecha</th>\n",
       "      <th>tipo_elem</th>\n",
       "      <th>intensidad</th>\n",
       "      <th>ocupacion</th>\n",
       "      <th>carga</th>\n",
       "      <th>vmed</th>\n",
       "      <th>error</th>\n",
       "      <th>periodo_integracion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>M30</td>\n",
       "      <td>2340</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "      <td>M30</td>\n",
       "      <td>2340</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "      <td>M30</td>\n",
       "      <td>2340</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "      <td>M30</td>\n",
       "      <td>2340</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>M30</td>\n",
       "      <td>2340</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                fecha tipo_elem  intensidad  ocupacion  carga  vmed  \\\n",
       "0  1001  2019-01-01 00:00:00       M30        2340       11.0      0  63.0   \n",
       "1  1001  2019-01-01 00:15:00       M30        2340       11.0      0  63.0   \n",
       "2  1001  2019-01-01 00:30:00       M30        2340       11.0      0  63.0   \n",
       "3  1001  2019-01-01 00:45:00       M30        2340       11.0      0  63.0   \n",
       "4  1001  2019-01-01 01:00:00       M30        2340       11.0      0  63.0   \n",
       "\n",
       "  error  periodo_integracion  \n",
       "0     N                    5  \n",
       "1     N                    5  \n",
       "2     N                    5  \n",
       "3     N                    5  \n",
       "4     N                    5  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se revisan las primeras filas del conjunto de datos\n",
    "trafico_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado que los ficheros de histórico de tráfico son muy pesados, de más de 500 MB, para añadir los datos históricos de tráfico\n",
    "# a cada accidente, se va a optar por hacer dos bucles anidados. En el primero de ellos, mes a mes se descarga el histórico de tráfico y carga\n",
    "# los puntos de medida y los accidentes del mes en cuestión. El siguiente bucle es para bajar a nivel fecha_hora dentro de un mismo mes, de tal forma\n",
    "# que se identifique para cada accidente, el punto de medida de tráfico más próximo en funcionamiento, y se añada dicha información\n",
    "# al dataframe de accidentes\n",
    "\n",
    "# Como paso inicial, se define la lista de fechas y el id del fichero csv para el primer bucle\n",
    "fecha_inicio = datetime(2019, 1, 1)\n",
    "fecha_fin = datetime(2023, 6, 1)\n",
    "lista_AnoMes = [dt.strftime('%Y-%m') for dt in (fecha_inicio + timedelta(days=i) for i in range((fecha_fin - fecha_inicio).days+1)) if dt.day == 1]\n",
    "lista_id_csv = range(68,122) # Desde enero de 2019 a junio de 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación, se define una función para invertir un diccionario, ya que se necesita invertir el diccionario utilizado para cargar los puntos de medida mes a mes\n",
    "def invertir_diccionario(diccionario):\n",
    "    diccionario_invertido = {valor: clave for clave, valor in diccionario.items()}\n",
    "    return diccionario_invertido\n",
    "\n",
    "# Se define el diccionario invertido de carga de los puntos de medida\n",
    "# Esto será de necesario para cargar solo los puntos de medida del mes en cuestión según el primer bucle\n",
    "dicc_pmed = invertir_diccionario(diccionario_pmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El siguiente paso consiste en definir una función que permita identificar el punto de medida más cercano a cada accidente para una fecha_hora determinada\n",
    "# Esta función se utilizará en el segundo bucle, ya que hay que hacer una búsqueda por fecha porque los puntos de medida más próximos pueden variar en función de la fecha,\n",
    "# ya sea porque se han movido o porque están inactivos\n",
    "def encuentra_pmed_mas_cercano(row, df2):\n",
    "    df_misma_fecha = df2[df2['fecha'] == row['fecha_hora']]\n",
    "    if df_misma_fecha.empty:\n",
    "        return None\n",
    "    dist_matrix = distance_matrix(row[['coordenada_x_utm', 'coordenada_y_utm']].values.reshape(1, -1), df_misma_fecha[['utm_x', 'utm_y']])\n",
    "    id_pmed_mas_cercano = dist_matrix.argmin()\n",
    "\n",
    "    return df_misma_fecha.iloc[id_pmed_mas_cercano]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seguimiento fecha:  2023-01-02 07:45:00 2023-01\n",
      "Seguimiento fecha:  2023-01-03 13:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-03 15:45:00 2023-01\n",
      "Seguimiento fecha:  2023-01-03 22:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-04 11:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-04 11:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-04 20:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-05 17:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-06 11:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-06 11:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-08 20:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-08 20:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-09 12:45:00 2023-01\n",
      "Seguimiento fecha:  2023-01-09 13:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-09 15:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-10 09:45:00 2023-01\n",
      "Seguimiento fecha:  2023-01-12 07:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-15 10:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-15 13:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-16 21:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-19 18:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-20 22:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-20 23:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-21 10:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-22 19:45:00 2023-01\n",
      "Seguimiento fecha:  2023-01-24 19:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-25 21:45:00 2023-01\n",
      "Seguimiento fecha:  2023-01-26 01:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-26 17:15:00 2023-01\n",
      "Seguimiento fecha:  2023-01-27 15:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-27 18:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-28 13:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-28 20:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-29 12:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-30 14:30:00 2023-01\n",
      "Seguimiento fecha:  2023-01-31 17:00:00 2023-01\n",
      "Seguimiento fecha:  2023-01-31 19:30:00 2023-01\n",
      "Seguimiento anomes:  2023-01 01-2023.csv\n",
      "Seguimiento fecha:  2023-02-01 14:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-03 15:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-04 17:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-05 11:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-06 08:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-06 14:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-07 07:15:00 2023-02\n",
      "Seguimiento fecha:  2023-02-07 17:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-07 19:15:00 2023-02\n",
      "Seguimiento fecha:  2023-02-07 21:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-09 09:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-09 21:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-10 14:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-10 22:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-11 13:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-11 16:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-12 07:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-12 15:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-13 14:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-13 19:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-13 20:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-15 14:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-15 02:15:00 2023-02\n",
      "Seguimiento fecha:  2023-02-16 10:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-16 17:15:00 2023-02\n",
      "Seguimiento fecha:  2023-02-16 21:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-17 16:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-17 20:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-17 22:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-18 00:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-19 11:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-20 09:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-20 09:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-21 13:15:00 2023-02\n",
      "Seguimiento fecha:  2023-02-22 09:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-23 08:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-22 13:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-23 18:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-23 21:15:00 2023-02\n",
      "Seguimiento fecha:  2023-02-25 00:45:00 2023-02\n",
      "Seguimiento fecha:  2023-02-26 16:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-26 16:00:00 2023-02\n",
      "Seguimiento fecha:  2023-02-27 20:30:00 2023-02\n",
      "Seguimiento fecha:  2023-02-28 13:15:00 2023-02\n",
      "Seguimiento anomes:  2023-02 02-2023.csv\n",
      "Seguimiento fecha:  2023-03-01 15:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-01 22:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-02 01:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-02 13:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-03 09:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-03 20:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-04 10:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-04 16:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-05 12:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-05 12:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-06 19:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-07 09:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-08 09:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-09 17:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-09 21:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-09 23:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-10 09:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-10 23:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-11 20:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-12 17:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-13 06:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-14 07:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-14 13:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-15 18:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-15 18:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-16 01:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-16 09:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-16 14:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-17 00:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-17 09:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-18 09:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-19 12:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-20 13:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-20 14:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-20 19:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-20 20:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-21 10:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-21 19:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-22 12:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-22 17:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-24 11:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-24 15:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-24 20:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-25 01:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-25 12:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-25 15:30:00 2023-03\n",
      "Seguimiento fecha:  2023-03-26 14:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-26 17:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-26 18:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-27 00:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-27 21:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-27 00:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-28 16:45:00 2023-03\n",
      "Seguimiento fecha:  2023-03-30 10:15:00 2023-03\n",
      "Seguimiento fecha:  2023-03-31 06:00:00 2023-03\n",
      "Seguimiento fecha:  2023-03-31 12:15:00 2023-03\n",
      "Seguimiento anomes:  2023-03 03-2023.csv\n",
      "Seguimiento fecha:  2023-04-01 06:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-03 09:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-02 14:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-05 17:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-05 19:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-07 19:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-08 12:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-08 16:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-08 18:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-08 16:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-09 18:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-09 16:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-09 13:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-10 09:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-10 14:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-10 14:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-10 17:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-11 09:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-11 19:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-11 22:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-12 09:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-11 18:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-12 21:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-13 13:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-13 14:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-13 18:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-13 16:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-15 16:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-15 19:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-15 17:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-15 20:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-15 22:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-17 14:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-17 20:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-17 20:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-18 08:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-18 13:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-19 13:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-19 13:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-20 08:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-20 12:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-21 20:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-22 06:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-22 12:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-22 13:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-23 09:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-23 11:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-23 12:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-23 20:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-23 23:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-24 21:15:00 2023-04\n",
      "Seguimiento fecha:  2023-04-25 13:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-25 15:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-25 19:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-25 21:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-26 08:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-26 12:30:00 2023-04\n",
      "Seguimiento fecha:  2023-04-26 15:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-27 12:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-28 00:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-26 10:45:00 2023-04\n",
      "Seguimiento fecha:  2023-04-28 14:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-28 20:00:00 2023-04\n",
      "Seguimiento fecha:  2023-04-30 21:30:00 2023-04\n",
      "Seguimiento anomes:  2023-04 04-2023.csv\n",
      "Seguimiento fecha:  2023-05-01 09:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-01 13:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-01 18:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-02 11:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-03 07:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-03 17:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-03 18:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-03 18:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-03 19:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-03 19:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-04 16:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-04 21:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-05 07:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-05 12:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-05 15:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-06 16:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 03:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 08:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 10:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 11:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 11:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 18:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 20:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-07 22:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-08 08:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-09 01:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-09 18:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-10 18:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-12 18:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-12 21:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-13 11:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-14 14:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-15 13:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-15 22:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-12 02:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-16 10:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-16 14:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-16 16:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-16 19:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-17 20:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-19 09:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-19 09:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-19 17:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-19 18:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-20 15:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-19 20:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-20 20:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-20 13:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-21 10:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-21 13:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-21 22:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-21 23:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-21 13:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-22 20:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-22 22:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-24 08:45:00 2023-05\n",
      "Seguimiento fecha:  2023-05-24 22:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-26 01:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-27 04:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-27 12:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-27 13:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-27 20:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-27 23:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-28 01:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-28 11:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-28 15:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-28 20:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-28 22:00:00 2023-05\n",
      "Seguimiento fecha:  2023-05-29 15:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-30 20:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-31 15:15:00 2023-05\n",
      "Seguimiento fecha:  2023-05-30 18:30:00 2023-05\n",
      "Seguimiento fecha:  2023-05-31 21:00:00 2023-05\n",
      "Seguimiento anomes:  2023-05 05-2023.csv\n",
      "Seguimiento fecha:  2023-06-01 10:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-01 14:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-01 14:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-01 21:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-02 01:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-02 06:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-02 09:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-01 13:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-02 19:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-02 20:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-04 11:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-05 08:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-06 02:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-06 07:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-06 12:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-06 11:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-06 19:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-07 00:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-07 19:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-07 20:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-08 15:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-09 12:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-09 14:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-09 17:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-09 22:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-10 20:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-11 19:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-12 08:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-12 14:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-12 18:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-13 10:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-14 15:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-14 20:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-16 00:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-16 20:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-16 22:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-17 10:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-17 14:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-18 01:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-18 09:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-19 13:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-20 08:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-20 12:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-20 17:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-20 21:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-21 02:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-21 09:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-21 18:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-21 18:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-22 20:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-22 21:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-22 23:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-23 17:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-24 00:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-24 08:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-24 09:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-25 13:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-25 12:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-26 11:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-26 13:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-27 01:30:00 2023-06\n",
      "Seguimiento fecha:  2023-06-27 17:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-28 11:15:00 2023-06\n",
      "Seguimiento fecha:  2023-06-28 19:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-29 01:45:00 2023-06\n",
      "Seguimiento fecha:  2023-06-29 10:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-29 19:00:00 2023-06\n",
      "Seguimiento fecha:  2023-06-30 19:00:00 2023-06\n",
      "Seguimiento anomes:  2023-06 06-2023.csv\n"
     ]
    }
   ],
   "source": [
    "# El último paso previo al bucle es definir una lista vacía donde se cargarán los accidentes una vez añadida la información relativa al histórico de tráfico\n",
    "accidentes_trafico_list = []\n",
    "\n",
    "# Por último, se procede con el primer bucle que tiene en cuenta tanto el id del csv de los históricos de tráfico como el AnoMes correspondiente\n",
    "for id, anomes in zip(lista_id_csv[48:], lista_AnoMes[48:]):\n",
    "    response = requests.get(trafico_url.format(id=id))\n",
    "    # En este caso, al igual que en la prueba anterior, la descarga es un fichero zip\n",
    "    with zipfile.ZipFile(BytesIO(response.content), 'r') as zip_ref:\n",
    "        # Dado que solo hay un archivo csv en el zip, seleccionamos el primero\n",
    "        csv_file_name = zip_ref.namelist()[0]\n",
    "\n",
    "        # Leemos el archivo csv que hay dentro del zip\n",
    "        with zip_ref.open(csv_file_name) as csv_file:\n",
    "            # Cargamos el archivo csv que contiene el histórico de tráfico del mes AnoMes en cuestión directamente con pandas\n",
    "            df_temp_traf = pd.read_csv(csv_file, delimiter=';', encoding='latin-1')\n",
    "            # Se obtienen los puntos de medida del AnoMes en cuestion\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    df_temp_pmed = pd.read_csv(pmed_url.format(id=dicc_pmed[anomes]), delimiter=';', encoding=encoding, decimal='.')\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            # Hacemos un merge entre el histórico de tráfico y el de puntos de medida para georreferencial los registros de tráfico\n",
    "            df_temp = df_temp_traf.merge(df_temp_pmed, left_on=['id'], right_on=['id'], how='left')\n",
    "            # Se eliminan aquellos registros de df_temp que no tienen coordenadas\n",
    "            df_temp.dropna(subset=['utm_x','utm_y'], inplace=True)\n",
    "            df_temp['utm_x'] = df_temp['utm_x'].astype(float)\n",
    "            df_temp['utm_y'] = df_temp['utm_y'].astype(float)\n",
    "            # Se obtiene el dataframe de accidentes filtrando por el AnoMes en cuestion\n",
    "            df_temp_accidentes_anomes = accidentes_raw_df[accidentes_raw_df['AnoMes']==anomes]\n",
    "            # Se procede con el segundo bucle. En este caso, para cada AnoMes se hace un bucle sobre las distintas fecha_hora de los accidentes \n",
    "            # para así ver cuál es el registro de tráfico más próximo a cada accidente\n",
    "            for f in df_temp_accidentes_anomes['fecha_hora'].unique():\n",
    "                # Se crea un dataframe con los accidentes de la fecha_hora en cuestión\n",
    "                df_temp_accidentes = df_temp_accidentes_anomes[df_temp_accidentes_anomes[\"fecha_hora\"]==f].copy(deep=True)\n",
    "                # Se cambian de formato las columnas de coordenadas\n",
    "                df_temp_accidentes['coordenada_x_utm'] = df_temp_accidentes['coordenada_x_utm'].astype(float)\n",
    "                df_temp_accidentes['coordenada_y_utm'] = df_temp_accidentes['coordenada_y_utm'].astype(float)\n",
    "                # Se aplica la función definida anteriormente para obtener el punto de medida más cercado a cada accidente\n",
    "                df_temp_accidentes['id_pmed_mas_cercano'] = df_temp_accidentes.apply(encuentra_pmed_mas_cercano, axis=1, df2=df_temp)\n",
    "                # Se filtran aquellos accidentes que no tienen un punto de medida cercano activo\n",
    "                df_temp_accidentes_clean = df_temp_accidentes.dropna(subset=['id_pmed_mas_cercano'])\n",
    "                # Se muestra por pantalla si se han eliminado registros porque no hay ningún punto de medida cercano activo a los accidentes\n",
    "                if (df_temp_accidentes.shape[0] - df_temp_accidentes_clean.shape[0]) != 0:\n",
    "                    print(f\"Se han eliminado {df_temp_accidentes.shape[0] - df_temp_accidentes_clean.shape[0]} registros porque no tienen punto de medida cercano para el año-mes {anomes} y fecha {f}\")\n",
    "                # Para los accidentes que sí tienen un punto de medida cercano activo...\n",
    "                if not df_temp_accidentes_clean.empty:\n",
    "                    # ... se añada la información relativa al histórico de accidentes\n",
    "                    accidentes_pmed_df = df_temp_accidentes_clean.merge(df_temp, left_on=['fecha_hora', 'id_pmed_mas_cercano'], right_on=['fecha', 'id'], how='left')\n",
    "                    # Se añade el registro resultante de añadir a cada accidente el histórico de tráfico a la lista\n",
    "                    accidentes_trafico_list.append(accidentes_pmed_df)\n",
    "                    # Se muestra en pantalla la fecha que se está analizando para hacer un seguimiento de la ejecución del segundo bucle\n",
    "                    print(\"Seguimiento fecha: \", f, anomes)\n",
    "            # Se concatenan los elementos de la lista para obtener un único dataframe resultante\n",
    "            accidentes_trafico_df = pd.concat(accidentes_trafico_list)\n",
    "            # Se guarda el dataframe resultante en un csv a modo de copia de seguridad, ya que solo nos interesa conservar el último fichero guardado\n",
    "            accidentes_trafico_df.to_csv('./Datasets/accidentes_definitivo_hasta_'+anomes+'.csv', sep=';', encoding='latin-1', index=False)\n",
    "            # Se muestra en pantalla el AnoMes actual del primer bucle\n",
    "            print(\"Seguimiento anomes: \", anomes, csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidentes_trafico_df = pd.concat(backup+accidentes_trafico_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3921 entries, 0 to 0\n",
      "Data columns (total 55 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   num_expediente           3921 non-null   object \n",
      " 1   fecha_x                  3921 non-null   object \n",
      " 2   hora                     3921 non-null   object \n",
      " 3   localizacion             3921 non-null   object \n",
      " 4   numero                   3921 non-null   object \n",
      " 5   cod_distrito             3921 non-null   int64  \n",
      " 6   distrito_x               3921 non-null   object \n",
      " 7   tipo_accidente           3921 non-null   object \n",
      " 8   estado_meteorologico     3796 non-null   object \n",
      " 9   tipo_vehiculo            3921 non-null   object \n",
      " 10  tipo_persona             3921 non-null   object \n",
      " 11  rango_edad               3921 non-null   object \n",
      " 12  sexo                     3921 non-null   object \n",
      " 13  cod_lesividad            3399 non-null   float64\n",
      " 14  lesividad                3399 non-null   object \n",
      " 15  coordenada_x_utm         3921 non-null   object \n",
      " 16  coordenada_y_utm         3921 non-null   object \n",
      " 17  positiva_alcohol         3915 non-null   object \n",
      " 18  positiva_droga           7 non-null      float64\n",
      " 19  AnoMes                   3921 non-null   object \n",
      " 20  fecha_hora               3921 non-null   object \n",
      " 21  implicados               3921 non-null   int64  \n",
      " 22  implicados_pesados       3921 non-null   float64\n",
      " 23  implicados_turismos      3921 non-null   float64\n",
      " 24  implicados_motocicletas  3921 non-null   float64\n",
      " 25  implicados_bicicletas    3921 non-null   float64\n",
      " 26  implicados_otros         3921 non-null   float64\n",
      " 27  implicados_epac          3921 non-null   float64\n",
      " 28  implicados_ligeros       3921 non-null   float64\n",
      " 29  localizacion_1           3921 non-null   object \n",
      " 30  localizacion_2           3921 non-null   object \n",
      " 31  clase_via_1              3847 non-null   object \n",
      " 32  nombre_via_1             3921 non-null   object \n",
      " 33  numero_via_1             2043 non-null   object \n",
      " 34  clase_via_2              1612 non-null   object \n",
      " 35  nombre_via_2             1707 non-null   object \n",
      " 36  numero_via_2             15 non-null     object \n",
      " 37  id_pmed_mas_cercano      3921 non-null   int64  \n",
      " 38  id                       3921 non-null   int64  \n",
      " 39  fecha_y                  3921 non-null   object \n",
      " 40  tipo_elem_x              3921 non-null   object \n",
      " 41  intensidad               3921 non-null   int64  \n",
      " 42  ocupacion                3918 non-null   float64\n",
      " 43  carga                    3921 non-null   int64  \n",
      " 44  vmed                     3921 non-null   float64\n",
      " 45  error                    3915 non-null   object \n",
      " 46  periodo_integracion      3921 non-null   int64  \n",
      " 47  tipo_elem_y              3921 non-null   object \n",
      " 48  distrito_y               3920 non-null   float64\n",
      " 49  cod_cent                 3921 non-null   object \n",
      " 50  nombre                   3918 non-null   object \n",
      " 51  utm_x                    3921 non-null   float64\n",
      " 52  utm_y                    3921 non-null   float64\n",
      " 53  longitud                 3921 non-null   float64\n",
      " 54  latitud                  3921 non-null   float64\n",
      "dtypes: float64(16), int64(7), object(32)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "accidentes_trafico_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se concatenan los elementos de la lista para obtener un único dataframe resultante\n",
    "accidentes_trafico_df = pd.concat(accidentes_trafico_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidentes_trafico_df = pd.read_csv('./Datasets/accidentes_definitivo_2019_2023.csv', delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_expediente', 'fecha_x', 'hora', 'localizacion', 'numero',\n",
       "       'cod_distrito', 'distrito_x', 'tipo_accidente', 'estado_meteorologico',\n",
       "       'tipo_vehiculo', 'tipo_persona', 'rango_edad', 'sexo', 'cod_lesividad',\n",
       "       'lesividad', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_alcohol',\n",
       "       'positiva_droga', 'AnoMes', 'fecha_hora', 'localizacion_1',\n",
       "       'localizacion_2', 'clase_via_1', 'nombre_via_1', 'numero_via_1',\n",
       "       'clase_via_2', 'nombre_via_2', 'numero_via_2', 'id_pmed_mas_cercano',\n",
       "       'id', 'fecha_y', 'tipo_elem_x', 'intensidad', 'ocupacion', 'carga',\n",
       "       'vmed', 'error', 'periodo_integracion', 'tipo_elem_y', 'distrito_y',\n",
       "       'cod_cent', 'nombre', 'utm_x', 'utm_y', 'longitud', 'latitud',\n",
       "       'implicados', 'implicados_peatones', 'implicados_pesados',\n",
       "       'implicados_turismos', 'implicados_motocicletas',\n",
       "       'implicados_bicicletas', 'implicados_otros', 'implicados_epac',\n",
       "       'implicados_ligeros'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se analizan las columnas del dataframe definitivo, en aras de revisar cuáles hay que eliminar\n",
    "accidentes_trafico_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan las columnas innecesarias\n",
    "columnas_seleccionadas = ['num_expediente', 'fecha_x', 'hora', 'localizacion', 'numero',\n",
    "       'localizacion_1', 'clase_via_1', 'nombre_via_1', 'numero_via_1',\n",
    "       'localizacion_2', 'clase_via_2', 'nombre_via_2', 'numero_via_2',\n",
    "       'cod_distrito', 'distrito_x', 'tipo_accidente', 'estado_meteorologico',\n",
    "       'tipo_vehiculo', 'implicados', 'implicados_peatones', 'implicados_pesados', 'implicados_turismos',\n",
    "       'implicados_motocicletas', 'implicados_bicicletas', 'implicados_otros',\n",
    "       'implicados_epac', 'implicados_ligeros', 'tipo_persona', 'rango_edad', 'sexo', 'cod_lesividad',\n",
    "       'lesividad', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_alcohol', 'positiva_droga',\n",
    "       'fecha_hora', 'id_pmed_mas_cercano', 'tipo_elem_x', 'intensidad', 'ocupacion', 'carga', 'vmed',\n",
    "       'error', 'periodo_integracion', 'cod_cent', 'nombre', 'utm_x', 'utm_y']\n",
    "\n",
    "accidentes_trafico_df = accidentes_trafico_df[columnas_seleccionadas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombran las columnas\n",
    "columnas_renombradas = ['num_expediente', 'fecha_acc', 'hora_acc', 'localizacion_acc', 'numero_acc',\n",
    "       'localizacion_acc_1', 'clase_via_acc_1', 'nombre_via_acc_1', 'numero_via_acc_1',\n",
    "       'localizacion_acc_2', 'clase_via_acc_2', 'nombre_via_acc_2', 'numero_via_acc_2',\n",
    "       'cod_distrito', 'distrito', 'tipo_accidente', 'estado_meteorologico',\n",
    "       'tipo_vehiculo','implicados', 'implicados_peatones', 'implicados_pesados', 'implicados_turismos',\n",
    "       'implicados_motocicletas', 'implicados_bicicletas', 'implicados_otros',\n",
    "       'implicados_epac', 'implicados_ligeros', 'tipo_persona', 'rango_edad', 'sexo', 'cod_lesividad',\n",
    "       'lesividad', 'utm_x_acc', 'utm_y_acc', 'positiva_alcohol', 'positiva_droga',\n",
    "       'fecha_hora_med', 'id_pmed_mas_cercano', 'tipo_elem_pmed', 'intensidad',\n",
    "       'ocupacion', 'carga', 'vmed', 'error', 'periodo_integracion',\n",
    "       'cod_cent_pmed', 'nombre_pmed', 'utm_x_pmed', 'utm_y_pmed']\n",
    "accidentes_trafico_df.columns = columnas_renombradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el dataframe resultante en un csv\n",
    "accidentes_trafico_df.to_csv('./Datasets/accidentes_definitivo_2019_2023_v3.csv', sep=';', encoding='latin-1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3921 entries, 0 to 3920\n",
      "Data columns (total 49 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   num_expediente           3921 non-null   object \n",
      " 1   fecha_acc                3921 non-null   object \n",
      " 2   hora_acc                 3921 non-null   object \n",
      " 3   localizacion_acc         3921 non-null   object \n",
      " 4   numero_acc               3921 non-null   object \n",
      " 5   localizacion_acc_1       3921 non-null   object \n",
      " 6   clase_via_acc_1          3847 non-null   object \n",
      " 7   nombre_via_acc_1         3921 non-null   object \n",
      " 8   numero_via_acc_1         2043 non-null   object \n",
      " 9   localizacion_acc_2       3921 non-null   object \n",
      " 10  clase_via_acc_2          1612 non-null   object \n",
      " 11  nombre_via_acc_2         1707 non-null   object \n",
      " 12  numero_via_acc_2         15 non-null     object \n",
      " 13  cod_distrito             3921 non-null   int64  \n",
      " 14  distrito                 3921 non-null   object \n",
      " 15  tipo_accidente           3921 non-null   object \n",
      " 16  estado_meteorologico     3796 non-null   object \n",
      " 17  tipo_vehiculo            3921 non-null   object \n",
      " 18  implicados               3921 non-null   int64  \n",
      " 19  implicados_peatones      3921 non-null   float64\n",
      " 20  implicados_pesados       3921 non-null   float64\n",
      " 21  implicados_turismos      3921 non-null   float64\n",
      " 22  implicados_motocicletas  3921 non-null   float64\n",
      " 23  implicados_bicicletas    3921 non-null   float64\n",
      " 24  implicados_otros         3921 non-null   float64\n",
      " 25  implicados_epac          3921 non-null   float64\n",
      " 26  implicados_ligeros       3921 non-null   float64\n",
      " 27  tipo_persona             3921 non-null   object \n",
      " 28  rango_edad               3921 non-null   object \n",
      " 29  sexo                     3921 non-null   object \n",
      " 30  cod_lesividad            3399 non-null   float64\n",
      " 31  lesividad                3399 non-null   object \n",
      " 32  utm_x_acc                3921 non-null   float64\n",
      " 33  utm_y_acc                3921 non-null   float64\n",
      " 34  positiva_alcohol         3915 non-null   object \n",
      " 35  positiva_droga           7 non-null      float64\n",
      " 36  fecha_hora_med           3921 non-null   object \n",
      " 37  id_pmed_mas_cercano      3921 non-null   int64  \n",
      " 38  tipo_elem_pmed           3921 non-null   object \n",
      " 39  intensidad               3921 non-null   int64  \n",
      " 40  ocupacion                3918 non-null   float64\n",
      " 41  carga                    3921 non-null   int64  \n",
      " 42  vmed                     3921 non-null   float64\n",
      " 43  error                    3915 non-null   object \n",
      " 44  periodo_integracion      3921 non-null   int64  \n",
      " 45  cod_cent_pmed            3921 non-null   object \n",
      " 46  nombre_pmed              3918 non-null   object \n",
      " 47  utm_x_pmed               3921 non-null   float64\n",
      " 48  utm_y_pmed               3921 non-null   float64\n",
      "dtypes: float64(16), int64(6), object(27)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Observamos el tipo de dato de cada columna, así como el número de nulos\n",
    "accidentes_trafico_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_expediente</th>\n",
       "      <th>fecha_acc</th>\n",
       "      <th>hora_acc</th>\n",
       "      <th>localizacion_acc</th>\n",
       "      <th>numero_acc</th>\n",
       "      <th>localizacion_acc_1</th>\n",
       "      <th>clase_via_acc_1</th>\n",
       "      <th>nombre_via_acc_1</th>\n",
       "      <th>numero_via_acc_1</th>\n",
       "      <th>localizacion_acc_2</th>\n",
       "      <th>...</th>\n",
       "      <th>intensidad</th>\n",
       "      <th>ocupacion</th>\n",
       "      <th>carga</th>\n",
       "      <th>vmed</th>\n",
       "      <th>error</th>\n",
       "      <th>periodo_integracion</th>\n",
       "      <th>cod_cent_pmed</th>\n",
       "      <th>nombre_pmed</th>\n",
       "      <th>utm_x_pmed</th>\n",
       "      <th>utm_y_pmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019S000036</td>\n",
       "      <td>02/01/2019</td>\n",
       "      <td>20:45:00</td>\n",
       "      <td>AVDA GRAN VIA DE HORTALEZA / GTA LUIS ROSALES</td>\n",
       "      <td>65B</td>\n",
       "      <td>AVDA GRAN VIA DE HORTALEZA</td>\n",
       "      <td>AVENIDA</td>\n",
       "      <td>GRAN VIA DE HORTALEZA</td>\n",
       "      <td>None</td>\n",
       "      <td>GTA LUIS ROSALES</td>\n",
       "      <td>...</td>\n",
       "      <td>373</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>15</td>\n",
       "      <td>56002</td>\n",
       "      <td>C/. López de Hoyos - Guisona-Gran Vía de Horta...</td>\n",
       "      <td>444606.203129</td>\n",
       "      <td>4.479884e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019S000045</td>\n",
       "      <td>03/01/2019</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>CTRA DEHESA DE LA VILLA, 1</td>\n",
       "      <td>1</td>\n",
       "      <td>CTRA DEHESA DE LA VILLA, 1</td>\n",
       "      <td>CARRETERA</td>\n",
       "      <td>DEHESA DE LA VILLA1</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>14</td>\n",
       "      <td>29012</td>\n",
       "      <td>(TACTICO) AV. COMPLUTENSE S-N (GIRO A DEHESA D...</td>\n",
       "      <td>438132.271587</td>\n",
       "      <td>4.478716e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019S000132</td>\n",
       "      <td>03/01/2019</td>\n",
       "      <td>12:45:00</td>\n",
       "      <td>AVDA SANTA EUGENIA / CALL REAL DE ARGANDA</td>\n",
       "      <td>64</td>\n",
       "      <td>AVDA SANTA EUGENIA</td>\n",
       "      <td>AVENIDA</td>\n",
       "      <td>SANTA EUGENIA</td>\n",
       "      <td>None</td>\n",
       "      <td>CALL REAL DE ARGANDA</td>\n",
       "      <td>...</td>\n",
       "      <td>415</td>\n",
       "      <td>8.0</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>15</td>\n",
       "      <td>88006</td>\n",
       "      <td>C/. Peñaranda Bracamonte - Av. Entrepeñas-Av. ...</td>\n",
       "      <td>448457.948911</td>\n",
       "      <td>4.469188e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019S000132</td>\n",
       "      <td>03/01/2019</td>\n",
       "      <td>12:45:00</td>\n",
       "      <td>AVDA SANTA EUGENIA / CALL REAL DE ARGANDA</td>\n",
       "      <td>64</td>\n",
       "      <td>AVDA SANTA EUGENIA</td>\n",
       "      <td>AVENIDA</td>\n",
       "      <td>SANTA EUGENIA</td>\n",
       "      <td>None</td>\n",
       "      <td>CALL REAL DE ARGANDA</td>\n",
       "      <td>...</td>\n",
       "      <td>415</td>\n",
       "      <td>8.0</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>15</td>\n",
       "      <td>88006</td>\n",
       "      <td>C/. Peñaranda Bracamonte - Av. Entrepeñas-Av. ...</td>\n",
       "      <td>448457.948911</td>\n",
       "      <td>4.469188e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019S000133</td>\n",
       "      <td>03/01/2019</td>\n",
       "      <td>14:30:00</td>\n",
       "      <td>CALL FELIPE ALVAREZ, 10</td>\n",
       "      <td>10</td>\n",
       "      <td>CALL FELIPE ALVAREZ, 10</td>\n",
       "      <td>CALLE</td>\n",
       "      <td>FELIPE ALVAREZ</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>428</td>\n",
       "      <td>52.0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>15</td>\n",
       "      <td>82027</td>\n",
       "      <td>C/. Felipe Alvarez - Manuel Pavia-Jesus del Pino</td>\n",
       "      <td>447076.418439</td>\n",
       "      <td>4.470348e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  num_expediente   fecha_acc  hora_acc  \\\n",
       "0    2019S000036  02/01/2019  20:45:00   \n",
       "1    2019S000045  03/01/2019  10:30:00   \n",
       "2    2019S000132  03/01/2019  12:45:00   \n",
       "3    2019S000132  03/01/2019  12:45:00   \n",
       "4    2019S000133  03/01/2019  14:30:00   \n",
       "\n",
       "                                localizacion_acc numero_acc  \\\n",
       "0  AVDA GRAN VIA DE HORTALEZA / GTA LUIS ROSALES        65B   \n",
       "1                     CTRA DEHESA DE LA VILLA, 1          1   \n",
       "2      AVDA SANTA EUGENIA / CALL REAL DE ARGANDA         64   \n",
       "3      AVDA SANTA EUGENIA / CALL REAL DE ARGANDA         64   \n",
       "4                        CALL FELIPE ALVAREZ, 10         10   \n",
       "\n",
       "           localizacion_acc_1 clase_via_acc_1       nombre_via_acc_1  \\\n",
       "0  AVDA GRAN VIA DE HORTALEZA         AVENIDA  GRAN VIA DE HORTALEZA   \n",
       "1  CTRA DEHESA DE LA VILLA, 1       CARRETERA    DEHESA DE LA VILLA1   \n",
       "2          AVDA SANTA EUGENIA         AVENIDA          SANTA EUGENIA   \n",
       "3          AVDA SANTA EUGENIA         AVENIDA          SANTA EUGENIA   \n",
       "4     CALL FELIPE ALVAREZ, 10           CALLE         FELIPE ALVAREZ   \n",
       "\n",
       "  numero_via_acc_1    localizacion_acc_2  ... intensidad ocupacion carga  \\\n",
       "0             None      GTA LUIS ROSALES  ...        373       8.0    16   \n",
       "1             None                        ...         60       6.0     7   \n",
       "2             None  CALL REAL DE ARGANDA  ...        415       8.0    48   \n",
       "3             None  CALL REAL DE ARGANDA  ...        415       8.0    48   \n",
       "4               10                        ...        428      52.0    62   \n",
       "\n",
       "   vmed error periodo_integracion cod_cent_pmed  \\\n",
       "0   0.0     N                  15         56002   \n",
       "1   0.0     N                  14         29012   \n",
       "2   0.0     N                  15         88006   \n",
       "3   0.0     N                  15         88006   \n",
       "4   0.0     N                  15         82027   \n",
       "\n",
       "                                         nombre_pmed     utm_x_pmed  \\\n",
       "0  C/. López de Hoyos - Guisona-Gran Vía de Horta...  444606.203129   \n",
       "1  (TACTICO) AV. COMPLUTENSE S-N (GIRO A DEHESA D...  438132.271587   \n",
       "2  C/. Peñaranda Bracamonte - Av. Entrepeñas-Av. ...  448457.948911   \n",
       "3  C/. Peñaranda Bracamonte - Av. Entrepeñas-Av. ...  448457.948911   \n",
       "4   C/. Felipe Alvarez - Manuel Pavia-Jesus del Pino  447076.418439   \n",
       "\n",
       "     utm_y_pmed  \n",
       "0  4.479884e+06  \n",
       "1  4.478716e+06  \n",
       "2  4.469188e+06  \n",
       "3  4.469188e+06  \n",
       "4  4.470348e+06  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se previsualiza el dataframe resultante\n",
    "accidentes_trafico_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URB    3593\n",
       "M30     311\n",
       "C30      17\n",
       "Name: tipo_elem_pmed, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se analiza cuántos accidentes son urbanos y cuántos interurbanos\n",
    "accidentes_trafico_df.tipo_elem_pmed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un 2.4141% de los registros no tiene información de tráfico\n"
     ]
    }
   ],
   "source": [
    "# Se calcula qué porcentaje de los accidentes no tiene información relativa al tráfico\n",
    "print(f\"Un {((accidentes_raw_df.count()[0] - accidentes_trafico_df.count()[0]) / accidentes_raw_df.count()[0])*100:.4f}% de los registros no tiene información de tráfico\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
